---
layout: default
---

<!-- <figure>
    <center><img src="https://offline-rl.github.io/assets/OFFLINE_RL.gif" style="width:65%"; /></center>
    <figcaption style='text-align: center'><small>Source: <a href="https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html">Google AI Blog </a></small></figcaption>
</figure> -->




<div class="row">

<!-- <p style="color:red;"> The website for <b>2<sup>nd</sup></b> offline RL workshop at NeurIPS 2021 can be found at <a href=https://offline-rl-neurips.github.io/2021> offline-rl-neurips.github.io/2021</a>.<p> -->

<h3>Summary</h3>


<p>This workshop is about decision awareness in RL, which we refer to as the knowledge that each of the components of an RL system should be explicitly trained to help the agent take the optimal action. Classical examples of decision awareness include:</p> 
<ul> 
<li>Learning aspects of the dynamics that are important for decision making and not modelling irrelevant parts of the world</li> 
<li>Training a critic to directly aid policy optimization instead of focusing on return prediction only</li>
<li>Addressing the sequence of learning problems faced by an agent in its entirety instead of treating them in isolation</li>
<li>Meta-learning algorithmic components that optimize the learning progress itself</li>
</ul>

<p>For a list of representative papers, please visit the <a href="resources.html">resources page</a>.</p>

</div>

<div class="row">
<h3>Important Dates</h3>
<table class="table table-striped">
  <tbody>
    <tr>
      <td><a href="https://openreview.net/group?id=ICML.cc/2022/Workshop/DARL">Submission site</a> opens</td>
      <td>May 1, 2022</td>
    </tr>
    <tr>
      <td>Submission deadline</td>
      <td>May 27, 2022 AoE</td>
    </tr>
    <tr>
      <td>Decisions announced</td>
      <td>June 13, 2022</td>
    </tr>
    <tr>
      <td>Camera-ready and video submission due</td>
      <td>July 1, 2022 AoE</td>
    </tr>
    <tr>
      <td>Day of workshop</td>
      <td>July 22, 2022</td>
    </tr>
  </tbody>
</table>

</div>

<!-- <p>A reinforcement learning (RL) agent typically contains several modules such as a policy, a value function, or a model of the environment's dynamics. Conventionally, each of these modules is trained to perform well with respect to a criterion that does not explicitly consider its role, in interaction with other modules, in the eventual decision making of the agent. For instance, a model in a model-based RL agent is trained to be an accurate predictor of the environment dynamics; but accurate predictions may not be feasible or even necessary, for instance, when dealing with the irrelevant details of an image in a visual-based environment. Trying to learn an accurate model of the dynamics requires a much larger number of samples and higher-capacity function approximators compared to a <span><em>decision-aware</em></span> model that only focuses on modelling the relevant aspects of the environment. Decision awareness refers to the principle that each module should be trained to <span><em>explicitly</em></span> consider how its interaction with other modules leads to improving the agentâ€™s long-term performance.</p> -->
<!-- <p>Decision awareness goes beyond model learning. In actor-critic algorithms, a critic is trained to predict the expected return while later used to aid policy optimization. Is the accuracy on return prediction a sensible goal for critic learning? Or can we train a critic that considers its interaction with the policy and directly maximizes the resulting performance? More generally, what is the best way to learn each components of an RL agent, considering their mutual interactions and the eventual goal of maximizing rewards? Through this workshop, we seek answers to this question.</p> -->


<div id="PC" class="row">
<h3>Program Committee</h3>
<div class="break"></div>
	<ul style="width:25%; float:left; display: inline; ">
		<li>Akram Erraqabi</li>
		<li>Alberto Maria Metelli</li>
		<li>Alexander Grishin</li>
		<li>Adrien Ali Taiga</li>
		<li>Amarildo Likmeta</li>
		<li>Aneri Muni</li>
		<li>Ankesh Anand</li>
		<li>Annie S Chen</li>
		<li>Arsenii Kuznetsov</li>
		<li>Arushi Jain</li>
		<li>Brandon Cui</li>
		<li>Chris Lu</li>
		<li>Claas A Voelcker</li>
		<li>Clement Gehring</li>
		<li>Dilip Arumugam</li>
	 </ul>

	<ul style="width:25%; float:center; display: inline; ">
		<li>Eric Graves</li>
		<li>Haque Ishfaq</li>
		<li>Harley Wiltzer</li>
		<li>Jacob Buckman</li>
		<li>Janarthanan Rajendran</li>
		<li>Jesse Farebrother</li>
		<li>Khimya Khetarpal</li>
		<li>Kristopher De Asis</li>
		<li>Louis Kirsch</li>
		<li>Lucas Lehnert</li>
		<li>Mahan Fathi</li>
		<li>Mandana Samiei</li>
		<li>Manuel Del Verme</li>
		<li>Matteo Papini</li>
		<li>Matthia Sabatelli</li>
	</ul>

	<ul style="width:25%; float:right; display: inline;">
		<li>Maxime Heuillet</li>
		<li>Mehdi Fatemi</li>
		<li>Michel Ma</li>
		<li>Nikolaus H. R. Howe</li>
		<li>Raymond Chua</li>
		<li>Romina Abachi</li>
		<li>Safa Alver</li>
		<li>Shruti Joshi</li>
		<li>Tianwei Ni</li>
		<li>Timon Willi</li>
		<li>Vincent Mai</li>
		<li>Yi Wan</li>
		<li>Zhixuan Lin</li>
	</ul>
</div>

<div id="organizers" class="row">
<h3 style="float:left;">Organizers</h3>
<div class="break"></div>
<div style="text-align: left;">
{%- for person in site.data.organizers -%}
<div class="person">
  <img src="{{ person.image }}" height="170px" /><div style="height:12px;"></div>
   <a href="{{ person.url | relative_url }}">{{ person.name }}</a> <div style="height:4px;"></div>
   <span>{{ person.title | replace: '&', '<div style="height:4px;"></div>' }}</span>
</div>
{%- endfor -%}
</div>
</div>
<p> To contact the organizers, please send an email to <a href="mailto:darl.workshop@gmail.com">darl.workshop@gmail.com</a>. <p>


<p style="text-align:right">
  <small> Thanks to Rishabh Agarwal for providing this <a href="https://offline-rl-neurips.github.io/">template</a>. </small> </p>
